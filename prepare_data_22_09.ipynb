{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwMu90jutY1I",
        "outputId": "60ea5ab0-b1b7-48f4-8106-f18b57d1d532"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install beautifulsoup4 ftfy -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aipUIRNDuFIe",
        "outputId": "429ac4bb-e25d-431e-f1ad-2057e8424a1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpZdTDwro74t",
        "outputId": "12ec7c54-f50f-4d11-fe9c-ff78f4c6d2fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Phase 1: Reading and sanitizing '/content/drive/MyDrive/trn.json' ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-4060290164.py:25: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a URL than HTML or XML.\n",
            "\n",
            "If you meant to use Beautiful Soup to parse the web page found at a certain URL, then something has gone wrong. You should use an Python package like 'requests' to fetch the content behind the URL. Once you have the content as a string, you can feed that string into Beautiful Soup.\n",
            "\n",
            "However, if you want to parse some data that happens to look like a URL, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
            "\n",
            "    from bs4 import MarkupResemblesLocatorWarning\n",
            "    import warnings\n",
            "\n",
            "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
            "    \n",
            "  text = BeautifulSoup(text, \"html.parser\").get_text()\n",
            "/tmp/ipython-input-4060290164.py:25: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a filename than HTML or XML.\n",
            "\n",
            "If you meant to use Beautiful Soup to parse the contents of a file on disk, then something has gone wrong. You should open the file first, using code like this:\n",
            "\n",
            "    filehandle = open(your filename)\n",
            "\n",
            "You can then feed the open filehandle into Beautiful Soup instead of using the filename.\n",
            "\n",
            "However, if you want to parse some data that happens to look like a filename, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
            "\n",
            "    from bs4 import MarkupResemblesLocatorWarning\n",
            "    import warnings\n",
            "\n",
            "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
            "    \n",
            "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scanned 200000 lines...\n",
            "Scanned 400000 lines...\n",
            "Scanned 600000 lines...\n",
            "Scanned 800000 lines...\n",
            "Scanned 1000000 lines...\n",
            "Scanned 1200000 lines...\n",
            "Scanned 1400000 lines...\n",
            "Scanned 1600000 lines...\n",
            "Scanned 1800000 lines...\n",
            "Scanned 2000000 lines...\n",
            "Scanned 2200000 lines...\n",
            "✅ Phase 1 complete. Found 1390076 valid and sanitized records.\n",
            "\n",
            "--- Phase 2: Selecting records to save ---\n",
            "Found more than 50000 records. Taking a random sample.\n",
            "✅ Phase 2 complete. Selected 50000 records.\n",
            "\n",
            "--- Phase 3: Writing 50000 records to '/content/drive/MyDrive/formatted_amazon_products_50k_sanitized.jsonl' ---\n",
            "\n",
            "----------------------------------------------------\n",
            "🎉 Success! A sanitized sample of 50000 records has been saved.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import ftfy\n",
        "\n",
        "input_file_path = '/content/drive/MyDrive/trn.json'\n",
        "output_file_path = '/content/drive/MyDrive/formatted_amazon_products_50k_sanitized.jsonl'\n",
        "num_records_to_save = 50000\n",
        "\n",
        "def sanitize_text(text):\n",
        "    \"\"\"\n",
        "    Cleans a string by fixing encoding, removing HTML, and normalizing whitespace.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    text = ftfy.fix_text(text)\n",
        "\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "\n",
        "    text = ' '.join(text.split()).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "valid_records = []\n",
        "\n",
        "print(f\"--- Phase 1: Reading and sanitizing '{input_file_path}' ---\")\n",
        "try:\n",
        "    with open(input_file_path, 'r', encoding='utf-8') as f_in:\n",
        "        for i, line in enumerate(f_in):\n",
        "            try:\n",
        "                data = json.loads(line)\n",
        "\n",
        "                if data.get('title') and data.get('content'):\n",
        "                    clean_title = sanitize_text(data['title'])\n",
        "                    clean_content = sanitize_text(data['content'])\n",
        "\n",
        "                    if clean_title and clean_content:\n",
        "                        new_record = {\n",
        "                            'title': clean_title,\n",
        "                            'content': clean_content\n",
        "                        }\n",
        "                        valid_records.append(new_record)\n",
        "\n",
        "                if (i + 1) % 200000 == 0:\n",
        "                    print(f\"Scanned {i+1} lines...\")\n",
        "\n",
        "            except (json.JSONDecodeError, TypeError):\n",
        "                continue\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ Error: The file '{input_file_path}' was not found.\")\n",
        "    exit()\n",
        "\n",
        "total_valid = len(valid_records)\n",
        "print(f\"✅ Phase 1 complete. Found {total_valid} valid and sanitized records.\\n\")\n",
        "\n",
        "print(f\"--- Phase 2: Selecting records to save ---\")\n",
        "final_records = []\n",
        "\n",
        "if total_valid == 0:\n",
        "    print(\"No valid records were found. The output file will be empty.\")\n",
        "elif total_valid > num_records_to_save:\n",
        "    print(f\"Found more than {num_records_to_save} records. Taking a random sample.\")\n",
        "    final_records = random.sample(valid_records, num_records_to_save)\n",
        "else:\n",
        "    print(f\"Found {total_valid} records, which is less than or equal to {num_records_to_save}. Saving all of them.\")\n",
        "    final_records = valid_records\n",
        "\n",
        "print(f\"✅ Phase 2 complete. Selected {len(final_records)} records.\\n\")\n",
        "\n",
        "print(f\"--- Phase 3: Writing {len(final_records)} records to '{output_file_path}' ---\")\n",
        "with open(output_file_path, 'w', encoding='utf-8') as f_out:\n",
        "    for record in final_records:\n",
        "        f_out.write(json.dumps(record) + '\\n')\n",
        "\n",
        "print(\"\\n----------------------------------------------------\")\n",
        "print(f\"🎉 Success! A sanitized sample of {len(final_records)} records has been saved.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
