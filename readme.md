# ğŸš€ Fine-Tuning de Modelo com 50k Dados

Este repositÃ³rio documenta o processo completo de **fine-tuning de um modelo de linguagem** utilizando **50.000 amostras de dados**.  
O pipeline inclui **preparaÃ§Ã£o do dataset**, **treinamento no Google Colab** e **comparaÃ§Ã£o do modelo fine-tuned com o modelo base**.  

---

## ğŸ“‚ Estrutura do RepositÃ³rio

- **`prepare_data_22_09.ipynb`**  
  ResponsÃ¡vel por preparar o dataset de entrada.  
  - Limpeza e padronizaÃ§Ã£o  
  - DivisÃ£o em treino e validaÃ§Ã£o  
  - ExportaÃ§Ã£o no formato compatÃ­vel com Hugging Face Datasets  

- **`fine_tuning_50k_colab_22_09.ipynb`**  
  Notebook que executa o fine-tuning do modelo.  
  - ConfiguraÃ§Ã£o do ambiente (Google Colab + GPU)  
  - Carregamento do modelo base (Hugging Face Transformers)  
  - Treinamento com 50k amostras  
  - Salvamento do modelo fine-tuned  

- **`compare_models_final_22_09.ipynb`**  
  AvaliaÃ§Ã£o e anÃ¡lise de desempenho.  
  - ComparaÃ§Ã£o entre modelo base e modelo fine-tuned  
  - MÃ©tricas como loss, acurÃ¡cia e perplexidade  
  - GrÃ¡ficos de evoluÃ§Ã£o do treinamento  
  - ConclusÃµes sobre os ganhos obtidos  

---

## âš™ï¸ Requisitos

- Python 3.8+  
- Google Colab ou ambiente com GPU CUDA  
- Bibliotecas principais:  
  - `transformers` (Hugging Face)  
  - `datasets`  
  - `torch`  
  - `pandas`, `numpy`, `matplotlib`  

Instale os pacotes necessÃ¡rios com:  

```bash
pip install torch transformers datasets pandas matplotlib
```

---

## ğŸš€ Como Reproduzir

1. Clone este repositÃ³rio:
   ```bash
   git clone https://github.com/seu-usuario/seu-repo.git
   cd seu-repo
   ```

2. Execute os notebooks na seguinte ordem:
   1. `prepare_data_22_09.ipynb` â†’ Gera o dataset final.  
   2. `fine_tuning_50k_colab_22_09.ipynb` â†’ Treina o modelo.  
   3. `compare_models_final_22_09.ipynb` â†’ Analisa os resultados.  

3. (Opcional) Publique o modelo no [Hugging Face Hub](https://huggingface.co/).  

---

## ğŸ“Š Resultados

O modelo treinado com **50.000 amostras** apresentou:  
- ReduÃ§Ã£o significativa do **loss** em relaÃ§Ã£o ao modelo base.  
- Melhoria nas mÃ©tricas de avaliaÃ§Ã£o (ex.: acurÃ¡cia e perplexidade).  
- Melhor desempenho em inferÃªncias de teste qualitativas.  

ğŸ‘‰ Os grÃ¡ficos e mÃ©tricas detalhadas estÃ£o no notebook `compare_models_final_22_09.ipynb`.

---

## ğŸ–¥ï¸ Exemplo de InferÃªncia

ApÃ³s o fine-tuning, o modelo pode ser utilizado para inferÃªncia da seguinte forma:

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

MODEL_PATH = "seu-usuario/seu-modelo-finetuned"

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)

prompt = "Explique de forma simples o que Ã© aprendizado de mÃ¡quina."

inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=100)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

---

## ğŸ“Œ PrÃ³ximos Passos

- [ ] Publicar modelo no Hugging Face Hub  
- [ ] Testar em **benchmark externo** para validaÃ§Ã£o mais robusta  
- [ ] Ampliar dataset (>100k amostras)  
- [ ] Experimentar tÃ©cnicas de **LoRA** e **PEFT** para fine-tuning mais eficiente  

---

## ğŸ“œ LicenÃ§a

Este projeto estÃ¡ licenciado sob a **MIT License** - veja o arquivo [LICENSE](LICENSE) para mais detalhes.